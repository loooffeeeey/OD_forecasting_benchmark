> Seed: 66666
> device: cuda:0
> Loading DataSet from data/ny2016_0101to0331/
> Total Hours: 2184, starting from 1
> Unify FB Graphs: False, Mix FB Graphs: False
> Training batches: 53, Validation batches: 6
> Initializing the Training Model: GallatExtFull, Train type = normal
> Model Structure:
GallatExtFull(
  (spatAttLayer): SpatAttLayer(
    (dimSpatAttLayers): ModuleList(
      (0): MultiHeadPwGaANLayer(
        (pwGaAN): PwGaANLayer(
          (Wa): Linear(in_features=43, out_features=16, bias=False)
          (att_out_fc_l): Linear(in_features=16, out_features=1, bias=False)
          (att_out_fc_r): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_l): Linear(in_features=43, out_features=1, bias=False)
          (gate_fc_m): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_r): Linear(in_features=43, out_features=1, bias=False)
          (Wg): Linear(in_features=43, out_features=16, bias=False)
        )
      )
      (1): MultiHeadPwGaANLayer(
        (pwGaAN): PwGaANLayer(
          (Wa): Linear(in_features=43, out_features=16, bias=False)
          (att_out_fc_l): Linear(in_features=16, out_features=1, bias=False)
          (att_out_fc_r): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_l): Linear(in_features=43, out_features=1, bias=False)
          (gate_fc_m): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_r): Linear(in_features=43, out_features=1, bias=False)
          (Wg): Linear(in_features=43, out_features=16, bias=False)
        )
      )
      (2): MultiHeadPwGaANLayer(
        (pwGaAN): PwGaANLayer(
          (Wa): Linear(in_features=43, out_features=16, bias=False)
          (att_out_fc_l): Linear(in_features=16, out_features=1, bias=False)
          (att_out_fc_r): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_l): Linear(in_features=43, out_features=1, bias=False)
          (gate_fc_m): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_r): Linear(in_features=43, out_features=1, bias=False)
          (Wg): Linear(in_features=43, out_features=16, bias=False)
        )
      )
    )
    (proj_fc): Linear(in_features=43, out_features=16, bias=False)
    (bn): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (tempLayer): TempRecurrentLayer(
    (recurrentBlocks): ModuleList(
      (0): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
      (1): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
      (2): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
      (3): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
    )
    (bn): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (tranAttLayer): TranAttLayer(
    (demand_fc): Linear(in_features=640, out_features=1, bias=True)
    (Wa): Linear(in_features=640, out_features=640, bias=False)
    (att_out_fc_l): Linear(in_features=640, out_features=1, bias=False)
    (att_out_fc_r): Linear(in_features=640, out_features=1, bias=False)
  )
)
> Constructing the Optimizer: ADAM
> Using SmoothL1Loss as the Loss Function.
> Model sent to cuda:0

learning_rate = 0.01, epochs = 200, num_workers = 20
eval_freq = 5, batch_size = 32, optimizer = ADAM
tune = True, use_AR=None, ref_extent = -1.00

Start Training!
------------------------------------------------------------------------
Training Round 1: loss = 21.831126, time_cost = 326.7344 sec (0.1946 sec per sample), RMSE-0 = 1362.7935, MAPE-0 = 3.7772, MAE-0 = 122.5155
Training Round 2: loss = 7.252676, time_cost = 309.9635 sec (0.1846 sec per sample), RMSE-0 = 41.4628, MAPE-0 = 0.4608, MAE-0 = 7.9271
Training Round 3: loss = 5.851443, time_cost = 302.8827 sec (0.1804 sec per sample), RMSE-0 = 33.1568, MAPE-0 = 0.4323, MAE-0 = 6.5949
Training Round 4: loss = 5.279807, time_cost = 307.0339 sec (0.1829 sec per sample), RMSE-0 = 27.9615, MAPE-0 = 0.4216, MAE-0 = 5.8127
Training Round 5: loss = 4.437486, time_cost = 306.0555 sec (0.1823 sec per sample), RMSE-0 = 27.7204, MAPE-0 = 0.4178, MAE-0 = 5.7292
!!! Validation : loss = 3.967673, RMSE-0 = 21.9444, MAPE-0 = 0.4033, MAE-0 = 5.0009
Training Round 6: loss = 4.149684, time_cost = 317.8633 sec (0.1893 sec per sample), RMSE-0 = 27.4435, MAPE-0 = 0.4180, MAE-0 = 5.6948
Training Round 7: loss = 3.639524, time_cost = 312.5914 sec (0.1862 sec per sample), RMSE-0 = 24.5493, MAPE-0 = 0.4104, MAE-0 = 5.2628
Training Round 8: loss = 3.716855, time_cost = 316.7867 sec (0.1887 sec per sample), RMSE-0 = 25.6624, MAPE-0 = 0.4151, MAE-0 = 5.4405
Training Round 9: loss = 3.496709, time_cost = 313.7893 sec (0.1869 sec per sample), RMSE-0 = 24.4058, MAPE-0 = 0.4107, MAE-0 = 5.2317
Training Round 10: loss = 3.466686, time_cost = 311.7574 sec (0.1857 sec per sample), RMSE-0 = 24.1569, MAPE-0 = 0.4202, MAE-0 = 5.2709
!!! Validation : loss = 2.710085, RMSE-0 = 22.9273, MAPE-0 = 0.3922, MAE-0 = 4.8956
Training Round 11: loss = 3.233280, time_cost = 304.0768 sec (0.1811 sec per sample), RMSE-0 = 24.2736, MAPE-0 = 0.4093, MAE-0 = 5.1966
Training Round 12: loss = 3.297140, time_cost = 311.3327 sec (0.1854 sec per sample), RMSE-0 = 24.0442, MAPE-0 = 0.4086, MAE-0 = 5.1846
Training Round 13: loss = 3.072493, time_cost = 304.5213 sec (0.1814 sec per sample), RMSE-0 = 22.5692, MAPE-0 = 0.4067, MAE-0 = 4.9443
Training Round 14: loss = 3.059951, time_cost = 305.4671 sec (0.1819 sec per sample), RMSE-0 = 22.4487, MAPE-0 = 0.4043, MAE-0 = 4.9645
Training Round 15: loss = 3.158116, time_cost = 299.3605 sec (0.1783 sec per sample), RMSE-0 = 24.0255, MAPE-0 = 0.4059, MAE-0 = 5.1686
!!! Validation : loss = 2.873967, RMSE-0 = 20.3192, MAPE-0 = 0.4017, MAE-0 = 4.7541
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 16: loss = 3.141913, time_cost = 325.9630 sec (0.1941 sec per sample), RMSE-0 = 23.0689, MAPE-0 = 0.4035, MAE-0 = 5.0750
Training Round 17: loss = 2.994365, time_cost = 313.1366 sec (0.1865 sec per sample), RMSE-0 = 22.3073, MAPE-0 = 0.4002, MAE-0 = 4.9424
Training Round 18: loss = 2.958747, time_cost = 300.5653 sec (0.1790 sec per sample), RMSE-0 = 21.9827, MAPE-0 = 0.4018, MAE-0 = 4.9262
Training Round 19: loss = 2.961895, time_cost = 316.7645 sec (0.1887 sec per sample), RMSE-0 = 21.7908, MAPE-0 = 0.3974, MAE-0 = 4.8818
Training Round 20: loss = 2.977440, time_cost = 307.7838 sec (0.1833 sec per sample), RMSE-0 = 22.2389, MAPE-0 = 0.4006, MAE-0 = 4.9835
!!! Validation : loss = 2.464241, RMSE-0 = 20.0715, MAPE-0 = 0.4161, MAE-0 = 4.8029
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 21: loss = 2.810099, time_cost = 304.2186 sec (0.1812 sec per sample), RMSE-0 = 20.8748, MAPE-0 = 0.3948, MAE-0 = 4.7858
Training Round 22: loss = 2.886067, time_cost = 305.6615 sec (0.1820 sec per sample), RMSE-0 = 21.5248, MAPE-0 = 0.3970, MAE-0 = 4.8725
Training Round 23: loss = 2.771184, time_cost = 303.9375 sec (0.1810 sec per sample), RMSE-0 = 20.2642, MAPE-0 = 0.3931, MAE-0 = 4.6982
Training Round 24: loss = 2.893961, time_cost = 320.0892 sec (0.1906 sec per sample), RMSE-0 = 21.7148, MAPE-0 = 0.3949, MAE-0 = 4.9067
Training Round 25: loss = 2.886870, time_cost = 302.4844 sec (0.1802 sec per sample), RMSE-0 = 21.1717, MAPE-0 = 0.3944, MAE-0 = 4.8464
!!! Validation : loss = 2.494755, RMSE-0 = 19.7423, MAPE-0 = 0.3929, MAE-0 = 4.4114
Training Round 26: loss = 2.784718, time_cost = 308.4739 sec (0.1837 sec per sample), RMSE-0 = 20.7376, MAPE-0 = 0.3945, MAE-0 = 4.7980
Training Round 27: loss = 2.828135, time_cost = 304.4235 sec (0.1813 sec per sample), RMSE-0 = 21.1381, MAPE-0 = 0.3967, MAE-0 = 4.8790
Training Round 28: loss = 2.834954, time_cost = 313.8313 sec (0.1869 sec per sample), RMSE-0 = 21.4500, MAPE-0 = 0.3937, MAE-0 = 4.9346
Training Round 29: loss = 2.706265, time_cost = 308.2953 sec (0.1836 sec per sample), RMSE-0 = 20.1354, MAPE-0 = 0.3929, MAE-0 = 4.7745
Training Round 30: loss = 2.741673, time_cost = 311.0703 sec (0.1853 sec per sample), RMSE-0 = 20.2265, MAPE-0 = 0.3945, MAE-0 = 4.7758
!!! Validation : loss = 2.494125, RMSE-0 = 18.2736, MAPE-0 = 0.4002, MAE-0 = 4.3939
Training Round 31: loss = 2.784555, time_cost = 306.1062 sec (0.1823 sec per sample), RMSE-0 = 20.9901, MAPE-0 = 0.3953, MAE-0 = 4.8167
Training Round 32: loss = 2.729631, time_cost = 302.9765 sec (0.1805 sec per sample), RMSE-0 = 20.3791, MAPE-0 = 0.3915, MAE-0 = 4.7863
Training Round 33: loss = 2.796910, time_cost = 304.4716 sec (0.1813 sec per sample), RMSE-0 = 21.4630, MAPE-0 = 0.3961, MAE-0 = 4.9202
Training Round 34: loss = 2.683643, time_cost = 304.1900 sec (0.1812 sec per sample), RMSE-0 = 20.3014, MAPE-0 = 0.3926, MAE-0 = 4.7783
Training Round 35: loss = 2.863643, time_cost = 302.6404 sec (0.1803 sec per sample), RMSE-0 = 21.8290, MAPE-0 = 0.3950, MAE-0 = 5.0084
!!! Validation : loss = 2.680067, RMSE-0 = 23.6180, MAPE-0 = 0.4221, MAE-0 = 5.1650
Training Round 36: loss = 2.674289, time_cost = 307.2032 sec (0.1830 sec per sample), RMSE-0 = 21.0383, MAPE-0 = 0.3988, MAE-0 = 4.8574
Training Round 37: loss = 2.705986, time_cost = 302.7574 sec (0.1803 sec per sample), RMSE-0 = 21.2424, MAPE-0 = 0.3944, MAE-0 = 4.9187
Training Round 38: loss = 2.728388, time_cost = 311.9196 sec (0.1858 sec per sample), RMSE-0 = 21.4258, MAPE-0 = 0.3984, MAE-0 = 5.0044
Training Round 39: loss = 2.778453, time_cost = 305.0500 sec (0.1817 sec per sample), RMSE-0 = 21.3192, MAPE-0 = 0.3972, MAE-0 = 4.9571
Training Round 40: loss = 2.645564, time_cost = 308.0484 sec (0.1835 sec per sample), RMSE-0 = 21.0756, MAPE-0 = 0.3996, MAE-0 = 4.9445
!!! Validation : loss = 2.643733, RMSE-0 = 19.0189, MAPE-0 = 0.4176, MAE-0 = 4.7199
Training Round 41: loss = 2.701001, time_cost = 307.3884 sec (0.1831 sec per sample), RMSE-0 = 20.8832, MAPE-0 = 0.3988, MAE-0 = 4.9013
Training Round 42: loss = 2.624964, time_cost = 302.7119 sec (0.1803 sec per sample), RMSE-0 = 20.5912, MAPE-0 = 0.3976, MAE-0 = 4.8938
Training Round 43: loss = 2.605722, time_cost = 303.6717 sec (0.1809 sec per sample), RMSE-0 = 20.5422, MAPE-0 = 0.4000, MAE-0 = 4.8716
Training Round 44: loss = 2.600007, time_cost = 303.9376 sec (0.1810 sec per sample), RMSE-0 = 20.4574, MAPE-0 = 0.4002, MAE-0 = 4.8598
Training Round 45: loss = 2.566453, time_cost = 306.6359 sec (0.1826 sec per sample), RMSE-0 = 20.3266, MAPE-0 = 0.3971, MAE-0 = 4.8298
!!! Validation : loss = 2.831580, RMSE-0 = 21.6389, MAPE-0 = 0.4073, MAE-0 = 5.0088
Training Round 46: loss = 2.658140, time_cost = 304.2448 sec (0.1812 sec per sample), RMSE-0 = 21.1937, MAPE-0 = 0.4007, MAE-0 = 4.9985
Training Round 47: loss = 2.563096, time_cost = 306.1207 sec (0.1823 sec per sample), RMSE-0 = 20.4466, MAPE-0 = 0.4005, MAE-0 = 4.8911
Training Round 48: loss = 2.596782, time_cost = 307.2029 sec (0.1830 sec per sample), RMSE-0 = 20.7022, MAPE-0 = 0.4020, MAE-0 = 4.9201
Training Round 49: loss = 2.553709, time_cost = 305.3754 sec (0.1819 sec per sample), RMSE-0 = 20.2140, MAPE-0 = 0.4047, MAE-0 = 4.9093
Training Round 50: loss = 2.531538, time_cost = 305.4173 sec (0.1819 sec per sample), RMSE-0 = 20.2886, MAPE-0 = 0.4016, MAE-0 = 4.8905
!!! Validation : loss = 2.463147, RMSE-0 = 19.3577, MAPE-0 = 0.4225, MAE-0 = 4.6048
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 51: loss = 2.514816, time_cost = 305.8435 sec (0.1822 sec per sample), RMSE-0 = 20.4172, MAPE-0 = 0.4078, MAE-0 = 4.9109
Training Round 52: loss = 2.647331, time_cost = 305.8529 sec (0.1822 sec per sample), RMSE-0 = 21.6667, MAPE-0 = 0.4096, MAE-0 = 5.1016
Training Round 53: loss = 2.606748, time_cost = 303.8694 sec (0.1810 sec per sample), RMSE-0 = 21.7889, MAPE-0 = 0.4077, MAE-0 = 5.0716
Training Round 54: loss = 2.562317, time_cost = 301.9026 sec (0.1798 sec per sample), RMSE-0 = 20.9605, MAPE-0 = 0.4085, MAE-0 = 4.9635
Training Round 55: loss = 2.563097, time_cost = 303.7461 sec (0.1809 sec per sample), RMSE-0 = 21.3005, MAPE-0 = 0.4092, MAE-0 = 5.0490
!!! Validation : loss = 2.469258, RMSE-0 = 21.9106, MAPE-0 = 0.4270, MAE-0 = 4.9956
Training Round 56: loss = 2.579988, time_cost = 328.3358 sec (0.1956 sec per sample), RMSE-0 = 21.9933, MAPE-0 = 0.4158, MAE-0 = 5.1687
Training Round 57: loss = 2.610778, time_cost = 325.5490 sec (0.1939 sec per sample), RMSE-0 = 21.4613, MAPE-0 = 0.4125, MAE-0 = 5.0656
Training Round 58: loss = 2.511378, time_cost = 323.3021 sec (0.1926 sec per sample), RMSE-0 = 20.6621, MAPE-0 = 0.4099, MAE-0 = 4.9792
Training Round 59: loss = 2.536665, time_cost = 325.6731 sec (0.1940 sec per sample), RMSE-0 = 21.8582, MAPE-0 = 0.4143, MAE-0 = 5.0775
Training Round 60: loss = 2.575169, time_cost = 330.2244 sec (0.1967 sec per sample), RMSE-0 = 21.2983, MAPE-0 = 0.4103, MAE-0 = 5.0294
!!! Validation : loss = 2.412080, RMSE-0 = 18.4018, MAPE-0 = 0.4186, MAE-0 = 4.4222
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 61: loss = 2.549568, time_cost = 323.8476 sec (0.1929 sec per sample), RMSE-0 = 21.5752, MAPE-0 = 0.4152, MAE-0 = 5.0862
Training Round 62: loss = 2.523463, time_cost = 323.7633 sec (0.1928 sec per sample), RMSE-0 = 20.2976, MAPE-0 = 0.4064, MAE-0 = 4.8927
Training Round 63: loss = 2.473610, time_cost = 328.2753 sec (0.1955 sec per sample), RMSE-0 = 21.0121, MAPE-0 = 0.4103, MAE-0 = 4.9819
Training Round 64: loss = 2.485312, time_cost = 327.3706 sec (0.1950 sec per sample), RMSE-0 = 20.8237, MAPE-0 = 0.4100, MAE-0 = 4.9917
Training Round 65: loss = 2.472250, time_cost = 320.4441 sec (0.1909 sec per sample), RMSE-0 = 20.6338, MAPE-0 = 0.4059, MAE-0 = 4.9487
!!! Validation : loss = 2.404062, RMSE-0 = 17.6725, MAPE-0 = 0.4101, MAE-0 = 4.3973
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 66: loss = 2.459846, time_cost = 326.5606 sec (0.1945 sec per sample), RMSE-0 = 21.1521, MAPE-0 = 0.4113, MAE-0 = 4.9705
Training Round 67: loss = 2.465996, time_cost = 301.5713 sec (0.1796 sec per sample), RMSE-0 = 21.1739, MAPE-0 = 0.4099, MAE-0 = 4.9851
Training Round 68: loss = 2.514658, time_cost = 305.0239 sec (0.1817 sec per sample), RMSE-0 = 21.4021, MAPE-0 = 0.4130, MAE-0 = 5.0413
Training Round 69: loss = 2.494503, time_cost = 314.1881 sec (0.1871 sec per sample), RMSE-0 = 21.2670, MAPE-0 = 0.4158, MAE-0 = 5.0518
Training Round 70: loss = 2.580177, time_cost = 314.0747 sec (0.1871 sec per sample), RMSE-0 = 22.3622, MAPE-0 = 0.4112, MAE-0 = 5.0924
!!! Validation : loss = 2.492643, RMSE-0 = 21.3732, MAPE-0 = 0.4211, MAE-0 = 4.8808
Training Round 71: loss = 2.472609, time_cost = 336.5912 sec (0.2005 sec per sample), RMSE-0 = 20.4972, MAPE-0 = 0.4026, MAE-0 = 4.8722
Training Round 72: loss = 2.527107, time_cost = 325.2792 sec (0.1937 sec per sample), RMSE-0 = 21.2383, MAPE-0 = 0.4151, MAE-0 = 5.0141
Training Round 73: loss = 2.460423, time_cost = 317.5742 sec (0.1891 sec per sample), RMSE-0 = 20.8567, MAPE-0 = 0.4099, MAE-0 = 4.9648
Training Round 74: loss = 2.443038, time_cost = 316.7559 sec (0.1887 sec per sample), RMSE-0 = 21.1901, MAPE-0 = 0.4113, MAE-0 = 4.9656
Training Round 75: loss = 2.514288, time_cost = 315.5198 sec (0.1879 sec per sample), RMSE-0 = 21.1230, MAPE-0 = 0.4121, MAE-0 = 5.0049
!!! Validation : loss = 2.364285, RMSE-0 = 20.2452, MAPE-0 = 0.4360, MAE-0 = 4.9530
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 76: loss = 2.531758, time_cost = 328.4009 sec (0.1956 sec per sample), RMSE-0 = 21.4506, MAPE-0 = 0.4112, MAE-0 = 4.9850
Training Round 77: loss = 2.466582, time_cost = 323.1930 sec (0.1925 sec per sample), RMSE-0 = 20.5607, MAPE-0 = 0.4058, MAE-0 = 4.8420
Training Round 78: loss = 2.463860, time_cost = 327.1707 sec (0.1949 sec per sample), RMSE-0 = 20.6263, MAPE-0 = 0.4111, MAE-0 = 4.9513
Training Round 79: loss = 2.448631, time_cost = 320.8170 sec (0.1911 sec per sample), RMSE-0 = 20.2881, MAPE-0 = 0.4078, MAE-0 = 4.8467
Training Round 80: loss = 2.521003, time_cost = 325.1951 sec (0.1937 sec per sample), RMSE-0 = 20.5749, MAPE-0 = 0.4073, MAE-0 = 4.8748
!!! Validation : loss = 2.671432, RMSE-0 = 19.0388, MAPE-0 = 0.4139, MAE-0 = 4.6956
Training Round 81: loss = 2.491615, time_cost = 313.7031 sec (0.1868 sec per sample), RMSE-0 = 21.0242, MAPE-0 = 0.4120, MAE-0 = 4.9859
Training Round 82: loss = 2.525984, time_cost = 326.0788 sec (0.1942 sec per sample), RMSE-0 = 21.1363, MAPE-0 = 0.4145, MAE-0 = 4.9958
Training Round 83: loss = 2.496412, time_cost = 316.6848 sec (0.1886 sec per sample), RMSE-0 = 20.6388, MAPE-0 = 0.4103, MAE-0 = 4.8782
Training Round 84: loss = 2.480045, time_cost = 320.1208 sec (0.1907 sec per sample), RMSE-0 = 20.3891, MAPE-0 = 0.4074, MAE-0 = 4.8456
Training Round 85: loss = 2.418629, time_cost = 328.5739 sec (0.1957 sec per sample), RMSE-0 = 20.8963, MAPE-0 = 0.4133, MAE-0 = 4.9452
!!! Validation : loss = 2.649410, RMSE-0 = 22.6820, MAPE-0 = 0.4287, MAE-0 = 5.2144
Training Round 86: loss = 2.499064, time_cost = 327.6506 sec (0.1951 sec per sample), RMSE-0 = 21.4747, MAPE-0 = 0.4118, MAE-0 = 4.9845
Training Round 87: loss = 2.495433, time_cost = 324.5634 sec (0.1933 sec per sample), RMSE-0 = 21.3557, MAPE-0 = 0.4141, MAE-0 = 5.0320
Training Round 88: loss = 2.448190, time_cost = 337.1827 sec (0.2008 sec per sample), RMSE-0 = 21.7306, MAPE-0 = 0.4212, MAE-0 = 5.0877
Training Round 89: loss = 2.510090, time_cost = 319.9119 sec (0.1905 sec per sample), RMSE-0 = 21.2511, MAPE-0 = 0.4135, MAE-0 = 5.0371
Training Round 90: loss = 2.507155, time_cost = 311.1644 sec (0.1853 sec per sample), RMSE-0 = 21.4987, MAPE-0 = 0.4146, MAE-0 = 5.0075
!!! Validation : loss = 2.328593, RMSE-0 = 20.0783, MAPE-0 = 0.4259, MAE-0 = 4.8935
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 91: loss = 2.404973, time_cost = 318.8398 sec (0.1899 sec per sample), RMSE-0 = 21.2322, MAPE-0 = 0.4191, MAE-0 = 4.9963
Training Round 92: loss = 2.468337, time_cost = 333.3628 sec (0.1985 sec per sample), RMSE-0 = 21.4810, MAPE-0 = 0.4179, MAE-0 = 4.9970
Training Round 93: loss = 2.475838, time_cost = 326.1753 sec (0.1943 sec per sample), RMSE-0 = 21.0310, MAPE-0 = 0.4092, MAE-0 = 4.9379
Training Round 94: loss = 2.418933, time_cost = 314.5517 sec (0.1873 sec per sample), RMSE-0 = 21.1598, MAPE-0 = 0.4215, MAE-0 = 5.0241
Training Round 95: loss = 2.408383, time_cost = 320.4609 sec (0.1909 sec per sample), RMSE-0 = 20.6659, MAPE-0 = 0.4158, MAE-0 = 4.9163
!!! Validation : loss = 2.221063, RMSE-0 = 19.2324, MAPE-0 = 0.4331, MAE-0 = 4.7444
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 96: loss = 2.456316, time_cost = 318.2263 sec (0.1895 sec per sample), RMSE-0 = 21.6243, MAPE-0 = 0.4184, MAE-0 = 5.1002
Training Round 97: loss = 2.564253, time_cost = 324.7431 sec (0.1934 sec per sample), RMSE-0 = 21.6637, MAPE-0 = 0.4135, MAE-0 = 5.0507
Training Round 98: loss = 2.430238, time_cost = 323.6258 sec (0.1927 sec per sample), RMSE-0 = 20.3239, MAPE-0 = 0.4032, MAE-0 = 4.8062
Training Round 99: loss = 2.404624, time_cost = 315.4428 sec (0.1879 sec per sample), RMSE-0 = 20.7047, MAPE-0 = 0.4150, MAE-0 = 4.9460
Training Round 100: loss = 2.387185, time_cost = 322.8161 sec (0.1923 sec per sample), RMSE-0 = 21.0337, MAPE-0 = 0.4123, MAE-0 = 4.9643
!!! Validation : loss = 2.717355, RMSE-0 = 19.3110, MAPE-0 = 0.4252, MAE-0 = 4.6556
Training Round 101: loss = 2.431604, time_cost = 327.6118 sec (0.1951 sec per sample), RMSE-0 = 20.9713, MAPE-0 = 0.4150, MAE-0 = 4.9326
Training Round 102: loss = 2.516173, time_cost = 326.5162 sec (0.1945 sec per sample), RMSE-0 = 21.5134, MAPE-0 = 0.4173, MAE-0 = 5.0327
Training Round 103: loss = 2.422368, time_cost = 322.1447 sec (0.1919 sec per sample), RMSE-0 = 21.1098, MAPE-0 = 0.4200, MAE-0 = 4.9858
Training Round 104: loss = 2.427938, time_cost = 330.5071 sec (0.1968 sec per sample), RMSE-0 = 21.0453, MAPE-0 = 0.4153, MAE-0 = 4.9341
Training Round 105: loss = 2.412906, time_cost = 321.1744 sec (0.1913 sec per sample), RMSE-0 = 21.2057, MAPE-0 = 0.4184, MAE-0 = 4.9802
!!! Validation : loss = 2.374940, RMSE-0 = 18.0849, MAPE-0 = 0.4137, MAE-0 = 4.3935
Training Round 106: loss = 2.416716, time_cost = 325.2819 sec (0.1937 sec per sample), RMSE-0 = 20.6202, MAPE-0 = 0.4179, MAE-0 = 4.8928
Training Round 107: loss = 2.379009, time_cost = 314.1731 sec (0.1871 sec per sample), RMSE-0 = 19.9532, MAPE-0 = 0.4117, MAE-0 = 4.8244
Training Round 108: loss = 2.482987, time_cost = 314.7574 sec (0.1875 sec per sample), RMSE-0 = 21.2415, MAPE-0 = 0.4094, MAE-0 = 4.9164
Training Round 109: loss = 2.464903, time_cost = 316.5960 sec (0.1886 sec per sample), RMSE-0 = 20.4829, MAPE-0 = 0.4131, MAE-0 = 4.8885
Training Round 110: loss = 2.486581, time_cost = 320.8614 sec (0.1911 sec per sample), RMSE-0 = 20.8035, MAPE-0 = 0.4197, MAE-0 = 4.9488
!!! Validation : loss = 2.335681, RMSE-0 = 18.6676, MAPE-0 = 0.4091, MAE-0 = 4.4151
Training Round 111: loss = 2.443112, time_cost = 324.8182 sec (0.1935 sec per sample), RMSE-0 = 20.4222, MAPE-0 = 0.4100, MAE-0 = 4.8203
Training Round 112: loss = 2.446382, time_cost = 318.8031 sec (0.1899 sec per sample), RMSE-0 = 21.1935, MAPE-0 = 0.4183, MAE-0 = 4.9112
Training Round 113: loss = 2.404766, time_cost = 316.9562 sec (0.1888 sec per sample), RMSE-0 = 20.6365, MAPE-0 = 0.4068, MAE-0 = 4.8521
Training Round 114: loss = 2.386611, time_cost = 321.4020 sec (0.1914 sec per sample), RMSE-0 = 20.1529, MAPE-0 = 0.4072, MAE-0 = 4.8460
Training Round 115: loss = 2.436988, time_cost = 324.9765 sec (0.1936 sec per sample), RMSE-0 = 20.1617, MAPE-0 = 0.4113, MAE-0 = 4.8382
!!! Validation : loss = 2.502322, RMSE-0 = 20.8460, MAPE-0 = 0.4226, MAE-0 = 4.8473
Training Round 116: loss = 2.425102, time_cost = 322.2617 sec (0.1919 sec per sample), RMSE-0 = 20.7356, MAPE-0 = 0.4134, MAE-0 = 4.8938
Training Round 117: loss = 2.471700, time_cost = 315.0716 sec (0.1877 sec per sample), RMSE-0 = 20.8875, MAPE-0 = 0.4072, MAE-0 = 4.8766
Training Round 118: loss = 2.477181, time_cost = 320.4305 sec (0.1908 sec per sample), RMSE-0 = 21.4712, MAPE-0 = 0.4198, MAE-0 = 5.0769
Training Round 119: loss = 2.366676, time_cost = 315.1703 sec (0.1877 sec per sample), RMSE-0 = 20.5422, MAPE-0 = 0.4153, MAE-0 = 4.8867
Training Round 120: loss = 2.394650, time_cost = 315.6367 sec (0.1880 sec per sample), RMSE-0 = 20.1593, MAPE-0 = 0.4142, MAE-0 = 4.8157
!!! Validation : loss = 2.517999, RMSE-0 = 19.0190, MAPE-0 = 0.4210, MAE-0 = 4.8420
Training Round 121: loss = 2.357111, time_cost = 332.9882 sec (0.1983 sec per sample), RMSE-0 = 19.5551, MAPE-0 = 0.4076, MAE-0 = 4.7212
Training Round 122: loss = 2.427007, time_cost = 341.0251 sec (0.2031 sec per sample), RMSE-0 = 20.7415, MAPE-0 = 0.4100, MAE-0 = 4.8683
Training Round 123: loss = 2.402754, time_cost = 311.8608 sec (0.1857 sec per sample), RMSE-0 = 20.9607, MAPE-0 = 0.4171, MAE-0 = 4.9294
Training Round 124: loss = 2.432871, time_cost = 323.5102 sec (0.1927 sec per sample), RMSE-0 = 20.5577, MAPE-0 = 0.4109, MAE-0 = 4.8276
Training Round 125: loss = 2.341421, time_cost = 319.5119 sec (0.1903 sec per sample), RMSE-0 = 20.4176, MAPE-0 = 0.4103, MAE-0 = 4.8464
!!! Validation : loss = 2.270240, RMSE-0 = 17.5383, MAPE-0 = 0.4197, MAE-0 = 4.4314
Training Round 126: loss = 2.382939, time_cost = 336.2433 sec (0.2003 sec per sample), RMSE-0 = 20.8250, MAPE-0 = 0.4143, MAE-0 = 4.8608
Training Round 127: loss = 2.364723, time_cost = 330.7056 sec (0.1970 sec per sample), RMSE-0 = 20.6637, MAPE-0 = 0.4184, MAE-0 = 4.8849
Training Round 128: loss = 2.377025, time_cost = 324.3467 sec (0.1932 sec per sample), RMSE-0 = 20.0461, MAPE-0 = 0.4142, MAE-0 = 4.8528
Training Round 129: loss = 2.372588, time_cost = 324.9824 sec (0.1936 sec per sample), RMSE-0 = 20.7834, MAPE-0 = 0.4142, MAE-0 = 4.9148
Training Round 130: loss = 2.343276, time_cost = 314.0342 sec (0.1870 sec per sample), RMSE-0 = 20.5350, MAPE-0 = 0.4147, MAE-0 = 4.8408
!!! Validation : loss = 2.286635, RMSE-0 = 18.4028, MAPE-0 = 0.4153, MAE-0 = 4.4646
Training Round 131: loss = 2.439954, time_cost = 315.6505 sec (0.1880 sec per sample), RMSE-0 = 20.3183, MAPE-0 = 0.4089, MAE-0 = 4.8197
Training Round 132: loss = 2.435323, time_cost = 313.7997 sec (0.1869 sec per sample), RMSE-0 = 20.1746, MAPE-0 = 0.4070, MAE-0 = 4.7986
Training Round 133: loss = 2.433938, time_cost = 318.7383 sec (0.1898 sec per sample), RMSE-0 = 20.7436, MAPE-0 = 0.4115, MAE-0 = 4.8965
Training Round 134: loss = 2.407480, time_cost = 324.6102 sec (0.1933 sec per sample), RMSE-0 = 20.7842, MAPE-0 = 0.4087, MAE-0 = 4.8206
Training Round 135: loss = 2.336124, time_cost = 321.4561 sec (0.1915 sec per sample), RMSE-0 = 19.9767, MAPE-0 = 0.4129, MAE-0 = 4.7653
!!! Validation : loss = 2.643432, RMSE-0 = 23.3561, MAPE-0 = 0.4590, MAE-0 = 5.3787
Training Round 136: loss = 2.404212, time_cost = 326.2229 sec (0.1943 sec per sample), RMSE-0 = 20.7121, MAPE-0 = 0.4185, MAE-0 = 4.8955
Training Round 137: loss = 2.409351, time_cost = 335.7258 sec (0.2000 sec per sample), RMSE-0 = 20.2047, MAPE-0 = 0.4122, MAE-0 = 4.8147
Training Round 138: loss = 2.470864, time_cost = 334.2202 sec (0.1991 sec per sample), RMSE-0 = 20.6001, MAPE-0 = 0.4111, MAE-0 = 4.8117
Training Round 139: loss = 2.385444, time_cost = 310.6657 sec (0.1850 sec per sample), RMSE-0 = 20.6651, MAPE-0 = 0.4133, MAE-0 = 4.8589
Training Round 140: loss = 2.420887, time_cost = 320.6877 sec (0.1910 sec per sample), RMSE-0 = 20.5403, MAPE-0 = 0.4182, MAE-0 = 4.8759
!!! Validation : loss = 2.255620, RMSE-0 = 18.6890, MAPE-0 = 0.4165, MAE-0 = 4.3804
Training Round 141: loss = 2.422604, time_cost = 325.6880 sec (0.1940 sec per sample), RMSE-0 = 20.0764, MAPE-0 = 0.4110, MAE-0 = 4.7843
Training Round 142: loss = 2.352348, time_cost = 311.8391 sec (0.1857 sec per sample), RMSE-0 = 20.7445, MAPE-0 = 0.4223, MAE-0 = 4.8808
Training Round 143: loss = 2.427774, time_cost = 326.4836 sec (0.1945 sec per sample), RMSE-0 = 20.7505, MAPE-0 = 0.4158, MAE-0 = 4.8670
Training Round 144: loss = 2.436198, time_cost = 320.6650 sec (0.1910 sec per sample), RMSE-0 = 21.0706, MAPE-0 = 0.4197, MAE-0 = 4.8883
Training Round 145: loss = 2.371939, time_cost = 335.1976 sec (0.1996 sec per sample), RMSE-0 = 20.4421, MAPE-0 = 0.4191, MAE-0 = 4.8312
!!! Validation : loss = 2.350772, RMSE-0 = 18.7058, MAPE-0 = 0.4180, MAE-0 = 4.4586
Training Round 146: loss = 2.417354, time_cost = 336.0026 sec (0.2001 sec per sample), RMSE-0 = 20.9243, MAPE-0 = 0.4179, MAE-0 = 4.8435
Training Round 147: loss = 2.350809, time_cost = 318.1737 sec (0.1895 sec per sample), RMSE-0 = 20.6291, MAPE-0 = 0.4192, MAE-0 = 4.8081
Training Round 148: loss = 2.373847, time_cost = 323.0549 sec (0.1924 sec per sample), RMSE-0 = 20.9932, MAPE-0 = 0.4222, MAE-0 = 4.9053
Training Round 149: loss = 2.365582, time_cost = 314.5134 sec (0.1873 sec per sample), RMSE-0 = 20.7470, MAPE-0 = 0.4150, MAE-0 = 4.8112
Training Round 150: loss = 2.435351, time_cost = 323.3675 sec (0.1926 sec per sample), RMSE-0 = 21.3089, MAPE-0 = 0.4222, MAE-0 = 4.9299
!!! Validation : loss = 2.243768, RMSE-0 = 18.6211, MAPE-0 = 0.4364, MAE-0 = 4.6718
Training Round 151: loss = 2.463761, time_cost = 324.2029 sec (0.1931 sec per sample), RMSE-0 = 21.1746, MAPE-0 = 0.4241, MAE-0 = 4.9881
Training Round 152: loss = 2.413648, time_cost = 321.4123 sec (0.1914 sec per sample), RMSE-0 = 20.9971, MAPE-0 = 0.4201, MAE-0 = 4.8864
Training Round 153: loss = 2.442219, time_cost = 318.1949 sec (0.1895 sec per sample), RMSE-0 = 21.1647, MAPE-0 = 0.4197, MAE-0 = 4.9190
Training Round 154: loss = 2.441912, time_cost = 321.5445 sec (0.1915 sec per sample), RMSE-0 = 21.8372, MAPE-0 = 0.4290, MAE-0 = 5.0396
Training Round 155: loss = 2.345632, time_cost = 314.7474 sec (0.1875 sec per sample), RMSE-0 = 21.1863, MAPE-0 = 0.4268, MAE-0 = 4.9479
!!! Validation : loss = 2.598553, RMSE-0 = 22.1531, MAPE-0 = 0.4426, MAE-0 = 5.3487
Training Round 156: loss = 2.561869, time_cost = 309.1775 sec (0.1841 sec per sample), RMSE-0 = 21.8512, MAPE-0 = 0.4292, MAE-0 = 5.0644
Training Round 157: loss = 2.469958, time_cost = 313.8502 sec (0.1869 sec per sample), RMSE-0 = 21.6283, MAPE-0 = 0.4232, MAE-0 = 4.9926
Training Round 158: loss = 2.476110, time_cost = 326.4547 sec (0.1944 sec per sample), RMSE-0 = 21.0751, MAPE-0 = 0.4205, MAE-0 = 4.9158
Training Round 159: loss = 2.463131, time_cost = 326.0103 sec (0.1942 sec per sample), RMSE-0 = 21.4926, MAPE-0 = 0.4263, MAE-0 = 4.9307
Training Round 160: loss = 2.395198, time_cost = 323.8382 sec (0.1929 sec per sample), RMSE-0 = 21.4204, MAPE-0 = 0.4280, MAE-0 = 4.9643
!!! Validation : loss = 2.395068, RMSE-0 = 19.9709, MAPE-0 = 0.4251, MAE-0 = 4.8435
Training Round 161: loss = 2.366407, time_cost = 324.1607 sec (0.1931 sec per sample), RMSE-0 = 20.9715, MAPE-0 = 0.4244, MAE-0 = 4.8773
Training Round 162: loss = 2.340971, time_cost = 319.9364 sec (0.1906 sec per sample), RMSE-0 = 20.7761, MAPE-0 = 0.4262, MAE-0 = 4.8899
Training Round 163: loss = 2.355894, time_cost = 312.9872 sec (0.1864 sec per sample), RMSE-0 = 20.8140, MAPE-0 = 0.4299, MAE-0 = 4.8916
Training Round 164: loss = 2.367909, time_cost = 316.4142 sec (0.1885 sec per sample), RMSE-0 = 21.0739, MAPE-0 = 0.4330, MAE-0 = 4.9671
Training Round 165: loss = 2.477524, time_cost = 313.7376 sec (0.1869 sec per sample), RMSE-0 = 22.2149, MAPE-0 = 0.4318, MAE-0 = 5.1629
!!! Validation : loss = 2.207567, RMSE-0 = 18.1388, MAPE-0 = 0.4114, MAE-0 = 4.3850
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 166: loss = 2.391332, time_cost = 318.7117 sec (0.1898 sec per sample), RMSE-0 = 21.7526, MAPE-0 = 0.4311, MAE-0 = 4.9648
Training Round 167: loss = 2.364379, time_cost = 316.9118 sec (0.1888 sec per sample), RMSE-0 = 21.2645, MAPE-0 = 0.4318, MAE-0 = 4.9815
Training Round 168: loss = 2.427939, time_cost = 326.4928 sec (0.1945 sec per sample), RMSE-0 = 21.6008, MAPE-0 = 0.4331, MAE-0 = 5.0363
Training Round 169: loss = 2.492090, time_cost = 324.8645 sec (0.1935 sec per sample), RMSE-0 = 22.1928, MAPE-0 = 0.4369, MAE-0 = 5.0985
Training Round 170: loss = 2.310462, time_cost = 317.5942 sec (0.1892 sec per sample), RMSE-0 = 21.1550, MAPE-0 = 0.4335, MAE-0 = 5.0022
!!! Validation : loss = 2.206972, RMSE-0 = 18.9505, MAPE-0 = 0.4426, MAE-0 = 4.7156
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
Training Round 171: loss = 2.363809, time_cost = 321.8663 sec (0.1917 sec per sample), RMSE-0 = 20.9933, MAPE-0 = 0.4355, MAE-0 = 5.0032
Training Round 172: loss = 2.401116, time_cost = 323.5898 sec (0.1927 sec per sample), RMSE-0 = 21.5540, MAPE-0 = 0.4296, MAE-0 = 4.9835
Training Round 173: loss = 2.419322, time_cost = 317.5425 sec (0.1891 sec per sample), RMSE-0 = 21.1232, MAPE-0 = 0.4313, MAE-0 = 4.9804
Training Round 174: loss = 2.387095, time_cost = 327.2237 sec (0.1949 sec per sample), RMSE-0 = 21.1343, MAPE-0 = 0.4304, MAE-0 = 4.9087
Training Round 175: loss = 2.461993, time_cost = 316.8493 sec (0.1887 sec per sample), RMSE-0 = 21.9324, MAPE-0 = 0.4312, MAE-0 = 5.0228
!!! Validation : loss = 2.373289, RMSE-0 = 19.7655, MAPE-0 = 0.4257, MAE-0 = 4.7046
Training Round 176: loss = 2.334781, time_cost = 318.8103 sec (0.1899 sec per sample), RMSE-0 = 21.3182, MAPE-0 = 0.4264, MAE-0 = 4.9061
Training Round 177: loss = 2.372831, time_cost = 324.3102 sec (0.1932 sec per sample), RMSE-0 = 21.5340, MAPE-0 = 0.4294, MAE-0 = 4.9849
Training Round 178: loss = 2.356041, time_cost = 327.0142 sec (0.1948 sec per sample), RMSE-0 = 21.2313, MAPE-0 = 0.4267, MAE-0 = 4.8532
Training Round 179: loss = 2.372386, time_cost = 317.6871 sec (0.1892 sec per sample), RMSE-0 = 21.0339, MAPE-0 = 0.4291, MAE-0 = 4.9491
Training Round 180: loss = 2.400423, time_cost = 325.9939 sec (0.1942 sec per sample), RMSE-0 = 21.4190, MAPE-0 = 0.4342, MAE-0 = 5.0534
!!! Validation : loss = 2.285173, RMSE-0 = 19.9726, MAPE-0 = 0.4457, MAE-0 = 4.8695
Training Round 181: loss = 2.378811, time_cost = 317.9336 sec (0.1894 sec per sample), RMSE-0 = 21.1366, MAPE-0 = 0.4312, MAE-0 = 4.9249
Training Round 182: loss = 2.343794, time_cost = 322.6385 sec (0.1922 sec per sample), RMSE-0 = 20.7214, MAPE-0 = 0.4283, MAE-0 = 4.9180
Training Round 183: loss = 2.380353, time_cost = 319.9724 sec (0.1906 sec per sample), RMSE-0 = 21.5029, MAPE-0 = 0.4330, MAE-0 = 4.9842
Training Round 184: loss = 2.297212, time_cost = 320.0118 sec (0.1906 sec per sample), RMSE-0 = 21.7880, MAPE-0 = 0.4431, MAE-0 = 5.0401
Training Round 185: loss = 2.405175, time_cost = 318.6471 sec (0.1898 sec per sample), RMSE-0 = 21.1734, MAPE-0 = 0.4327, MAE-0 = 4.9822
!!! Validation : loss = 2.388328, RMSE-0 = 18.2545, MAPE-0 = 0.4304, MAE-0 = 4.7057
Training Round 186: loss = 2.382399, time_cost = 310.8752 sec (0.1852 sec per sample), RMSE-0 = 21.0760, MAPE-0 = 0.4364, MAE-0 = 5.0396
Training Round 187: loss = 2.438396, time_cost = 311.1600 sec (0.1853 sec per sample), RMSE-0 = 22.1116, MAPE-0 = 0.4350, MAE-0 = 5.0821
Training Round 188: loss = 2.406500, time_cost = 308.8523 sec (0.1840 sec per sample), RMSE-0 = 21.7499, MAPE-0 = 0.4359, MAE-0 = 4.9894
Training Round 189: loss = 2.291908, time_cost = 307.4929 sec (0.1831 sec per sample), RMSE-0 = 21.1462, MAPE-0 = 0.4333, MAE-0 = 4.9576
Training Round 190: loss = 2.365084, time_cost = 323.9110 sec (0.1929 sec per sample), RMSE-0 = 21.6402, MAPE-0 = 0.4384, MAE-0 = 4.9883
!!! Validation : loss = 2.372884, RMSE-0 = 20.4335, MAPE-0 = 0.4477, MAE-0 = 5.0032
Training Round 191: loss = 2.339545, time_cost = 322.3678 sec (0.1920 sec per sample), RMSE-0 = 20.5145, MAPE-0 = 0.4261, MAE-0 = 4.9054
Training Round 192: loss = 2.347781, time_cost = 315.4573 sec (0.1879 sec per sample), RMSE-0 = 21.4008, MAPE-0 = 0.4340, MAE-0 = 5.0172
Training Round 193: loss = 2.377412, time_cost = 318.2808 sec (0.1896 sec per sample), RMSE-0 = 21.8261, MAPE-0 = 0.4346, MAE-0 = 5.0139
Training Round 194: loss = 2.356061, time_cost = 315.0365 sec (0.1876 sec per sample), RMSE-0 = 22.4248, MAPE-0 = 0.4322, MAE-0 = 5.0817
Training Round 195: loss = 2.313818, time_cost = 323.0310 sec (0.1924 sec per sample), RMSE-0 = 21.0199, MAPE-0 = 0.4280, MAE-0 = 4.8934
!!! Validation : loss = 2.449993, RMSE-0 = 20.1849, MAPE-0 = 0.4486, MAE-0 = 4.9484
Training Round 196: loss = 2.330645, time_cost = 316.9561 sec (0.1888 sec per sample), RMSE-0 = 20.8419, MAPE-0 = 0.4326, MAE-0 = 4.9333
Training Round 197: loss = 2.379561, time_cost = 320.3555 sec (0.1908 sec per sample), RMSE-0 = 20.8226, MAPE-0 = 0.4316, MAE-0 = 4.9303
Training Round 198: loss = 2.332089, time_cost = 319.0483 sec (0.1900 sec per sample), RMSE-0 = 21.8614, MAPE-0 = 0.4387, MAE-0 = 5.1005
Training Round 199: loss = 2.369538, time_cost = 303.4289 sec (0.1807 sec per sample), RMSE-0 = 21.4520, MAPE-0 = 0.4351, MAE-0 = 4.9662
Training Round 200: loss = 2.301935, time_cost = 304.0983 sec (0.1811 sec per sample), RMSE-0 = 20.5351, MAPE-0 = 0.4336, MAE-0 = 4.9376
!!! Validation : loss = 2.200761, RMSE-0 = 18.7271, MAPE-0 = 0.4373, MAE-0 = 4.6552
Model: model_save/20220403_11_47_56.pth has been saved since it achieves smaller loss.
> Training finished.

> device: cuda:0
> Loading model_save/20220403_11_47_56.pth
> Model Structure:
GallatExtFull(
  (spatAttLayer): SpatAttLayer(
    (dimSpatAttLayers): ModuleList(
      (0): MultiHeadPwGaANLayer(
        (pwGaAN): PwGaANLayer(
          (Wa): Linear(in_features=43, out_features=16, bias=False)
          (att_out_fc_l): Linear(in_features=16, out_features=1, bias=False)
          (att_out_fc_r): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_l): Linear(in_features=43, out_features=1, bias=False)
          (gate_fc_m): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_r): Linear(in_features=43, out_features=1, bias=False)
          (Wg): Linear(in_features=43, out_features=16, bias=False)
        )
      )
      (1): MultiHeadPwGaANLayer(
        (pwGaAN): PwGaANLayer(
          (Wa): Linear(in_features=43, out_features=16, bias=False)
          (att_out_fc_l): Linear(in_features=16, out_features=1, bias=False)
          (att_out_fc_r): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_l): Linear(in_features=43, out_features=1, bias=False)
          (gate_fc_m): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_r): Linear(in_features=43, out_features=1, bias=False)
          (Wg): Linear(in_features=43, out_features=16, bias=False)
        )
      )
      (2): MultiHeadPwGaANLayer(
        (pwGaAN): PwGaANLayer(
          (Wa): Linear(in_features=43, out_features=16, bias=False)
          (att_out_fc_l): Linear(in_features=16, out_features=1, bias=False)
          (att_out_fc_r): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_l): Linear(in_features=43, out_features=1, bias=False)
          (gate_fc_m): Linear(in_features=16, out_features=1, bias=False)
          (gate_fc_r): Linear(in_features=43, out_features=1, bias=False)
          (Wg): Linear(in_features=43, out_features=16, bias=False)
        )
      )
    )
    (proj_fc): Linear(in_features=43, out_features=16, bias=False)
    (bn): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (tempLayer): TempRecurrentLayer(
    (recurrentBlocks): ModuleList(
      (0): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
      (1): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
      (2): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
      (3): RecurrentBlock(
        (blk_module): LSTM(160, 160)
      )
    )
    (bn): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (tranAttLayer): TranAttLayer(
    (demand_fc): Linear(in_features=640, out_features=1, bias=True)
    (Wa): Linear(in_features=640, out_features=640, bias=False)
    (att_out_fc_l): Linear(in_features=640, out_features=1, bias=False)
    (att_out_fc_r): Linear(in_features=640, out_features=1, bias=False)
  )
)
> Model sent to cuda:0
> Loading DataSet from data/ny2016_0101to0331/
> Total Hours: 2184, starting from 1
> Unify FB Graphs: False, Mix FB Graphs: False
> Validation batches: 6, Test batches: 11
tune = True, ref_extent = -1.00
> Metrics Evaluations for Validation Set:
Demand:
RMSE-0 = 109.9766, RMSE-3 = 145.0719, RMSE-5 = 146.7339
MAPE-0 = 0.6292, MAPE-3 = 0.5582, MAPE-5 = 0.4481
MAE-0 = 27.1864, MAE-3 = 46.5013, MAE-5 = 52.0407
OD:
RMSE-0 = 18.1948, RMSE-3 = 29.2345, RMSE-5 = 33.0186
MAPE-0 = 0.4354, MAPE-3 = 0.3785, MAPE-5 = 0.3450
MAE-0 = 4.6091, MAE-3 = 11.3159, MAE-5 = 14.0814
> Metrics Evaluations for Test Set:
Demand:
RMSE-0 = 91.4094, RMSE-3 = 121.0523, RMSE-5 = 129.4348
MAPE-0 = 0.4079, MAPE-3 = 0.3290, MAPE-5 = 0.2954
MAE-0 = 27.6920, MAE-3 = 47.6142, MAE-5 = 54.0222
OD:
RMSE-0 = 17.0450, RMSE-3 = 29.1927, RMSE-5 = 33.3987
MAPE-0 = 0.4128, MAPE-3 = 0.3759, MAPE-5 = 0.3487
MAE-0 = 4.5955, MAE-3 = 11.4526, MAE-5 = 14.2488
> Evaluation finished.
